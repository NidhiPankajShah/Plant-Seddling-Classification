{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "Seedling-Classification-CNN",
      "provenance": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVxeHZXHYmgK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! unzip -q test.zip\n",
        "! unzip -q train.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_St5zSGYd9n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "import cv2\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "species = ['Black-grass', 'Charlock', 'Cleavers', 'Common Chickweed', 'Common wheat', 'Fat Hen',\n",
        "          'Loose Silky-bent', 'Maize','Scentless Mayweed', 'Shepherds Purse',\n",
        "          'Small-flowered Cranesbill', 'Sugar beet']\n",
        "data_dir = './'\n",
        "train_dir = os.path.join(data_dir, 'train')\n",
        "test_dir = os.path.join(data_dir, 'test')\n",
        "\n",
        "train_data = []\n",
        "for species_id, sp in enumerate(species):\n",
        "    for file in os.listdir(os.path.join(train_dir, sp)):\n",
        "        train_data.append(['train/{}/{}'.format(sp, file), species_id, sp])\n",
        "        \n",
        "train = pd.DataFrame(train_data, columns=['File', 'SpeciesId','Species'])\n",
        "train.head()\n",
        "\n",
        "# Randomize the order of training set\n",
        "SEED = 42\n",
        "train = train.sample(frac=1, random_state=SEED) \n",
        "train.index = np.arange(len(train)) # Reset indices\n",
        "train.head()\n",
        "\n",
        "# Organize test files into DataFrame\n",
        "test_data = []\n",
        "for file in os.listdir(test_dir):\n",
        "    test_data.append(['test/{}'.format(file), file])\n",
        "test = pd.DataFrame(test_data, columns=['Filepath', 'File'])\n",
        "test.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u35OFnm6YgOa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "IMAGE_SIZE = 128\n",
        "\n",
        "def read_image(filepath):\n",
        "    return cv2.imread(os.path.join(data_dir, filepath))\n",
        "\n",
        "def noise_image(image):\n",
        "#     return image + cv2.randn(image,(0),(0))\n",
        "#     return cv2.randn(image, (5, 5, 5), (5, 5, 5));\n",
        "\n",
        "#     return cv2.GaussianBlur(image, (205, 205), 0)\n",
        "\n",
        "#     # Generate Gaussian noise\n",
        "#     gauss = np.random.normal(0, 1, image.size)\n",
        "#     gauss = gauss.reshape(image.shape[0], image.shape[1], image.shape[2]).astype('uint8')\n",
        "#     # Add the Gaussian noise to the image\n",
        "#     image_gauss = cv2.add(image, gauss)\n",
        "#     return image_gauss\n",
        "\n",
        "\n",
        "    # sigmas = 0.1 * image\n",
        "    sigmas = 0.5\n",
        "    randomNoise = np.random.randn(*image.shape) * sigmas\n",
        "    \n",
        "    randomNoise = randomNoise.astype('uint8')\n",
        "    output = cv2.add(image, randomNoise)\n",
        "    return output\n",
        "\n",
        "# Resize image to target size\n",
        "def resize_image(image, image_size):\n",
        "    return cv2.resize(image.copy(), image_size, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "# Image segmentation\n",
        "def create_mask(image):\n",
        "    # Convert from BGR to HSV color-space to extract colored object\n",
        "    image_hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "    # Define range of green in HSV\n",
        "#     lower_green = np.array([30, 100, 50])\n",
        "#     upper_green = np.array([85, 255, 255])\n",
        "    lower_green = np.array([30, 100, 50])\n",
        "    upper_green = np.array([85, 255, 255])\n",
        "    # Threshold the HSV image to get only green colors\n",
        "    mask = cv2.inRange(image_hsv, lower_green, upper_green)\n",
        "#     kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (15, 15))\n",
        "#     mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
        "    return mask\n",
        "\n",
        "def segment_image(image):\n",
        "    mask = create_mask(image)\n",
        "    res = cv2.bitwise_and(image, image, mask=mask)\n",
        "    return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dB1YDeqFY7gh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_total_train = np.zeros((train.shape[0], IMAGE_SIZE, IMAGE_SIZE, 3))\n",
        "X_total_noise_train = np.zeros((train.shape[0], IMAGE_SIZE, IMAGE_SIZE, 3))\n",
        "for i, file in tqdm(enumerate(train['File'].values)):\n",
        "    image = read_image(file)\n",
        "    # Blurring images\n",
        "    image_noised = noise_image(image)\n",
        "#     image_segmented = segment_image(image)\n",
        "#     X_train[i] = resize_image(image_segmented, (IMAGE_SIZE, IMAGE_SIZE))\n",
        "#     X_train[i] = resize_image(image, (IMAGE_SIZE, IMAGE_SIZE))\n",
        "    X_total_noise_train[i] = resize_image(image_noised, (IMAGE_SIZE, IMAGE_SIZE))\n",
        "    X_total_train[i] = resize_image(image, (IMAGE_SIZE, IMAGE_SIZE))\n",
        "    \n",
        "#     X_train[i] = resize_image(segment_image(image_noised), (IMAGE_SIZE, IMAGE_SIZE))\n",
        "\n",
        "# Normalize the data\n",
        "X_total_train = X_total_train / 255.\n",
        "X_total_noise_train = X_total_noise_train / 255.\n",
        "print('Train Shape: {}'.format(X_total_train.shape))\n",
        "print('Train Shape: {}'.format(X_total_noise_train.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMlgxqsxY8fk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_train = train['SpeciesId'].values\n",
        "Y_total_noise_train = to_categorical(Y_train, num_classes=12)\n",
        "Y_total_train = to_categorical(Y_train, num_classes=12)\n",
        "print(len(Y_total_noise_train))\n",
        "print(len(Y_total_train))\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 100\n",
        "\n",
        "# Split the train and validation sets \n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X_total_train, Y_total_train, test_size=0.1, random_state=SEED)\n",
        "\n",
        "X_noise_train, X_noise_val, Y_noise_train, Y_noise_val = train_test_split(X_total_noise_train, Y_total_noise_train, test_size=0.1, random_state=SEED)\n",
        "\n",
        "print(X_noise_val.shape)\n",
        "print(X_val.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4lyK13gZ7Bp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test = np.zeros((test.shape[0], IMAGE_SIZE, IMAGE_SIZE, 3))\n",
        "X_test_noise = np.zeros((test.shape[0], IMAGE_SIZE, IMAGE_SIZE, 3))\n",
        "for i, file in tqdm(enumerate(test['Filepath'].values)):\n",
        "    image = read_image(file)\n",
        "    image_noised = noise_image(image)\n",
        "#     image_segmented = segment_image(image)\n",
        "#     X_test[i] = resize_image(image_segmented, (IMAGE_SIZE, IMAGE_SIZE))\n",
        "#     X_test[i] = resize_image(image, (IMAGE_SIZE, IMAGE_SIZE))\n",
        "#     X_test[i] = resize_image(image_noised, (IMAGE_SIZE, IMAGE_SIZE))\n",
        "    X_test_noise[i] = resize_image(image_noised, (IMAGE_SIZE, IMAGE_SIZE))\n",
        "    X_test[i] = resize_image(image, (IMAGE_SIZE, IMAGE_SIZE))\n",
        "\n",
        "X_test = X_test / 255.\n",
        "\n",
        "X_test_noise = X_test_noise / 255."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jS3-Tvlca40s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Lambda, Input, Dense\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from keras import metrics\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Conv2DTranspose,Reshape\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization, Activation\n",
        "from keras.optimizers import Adam\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "image_size = X_train.shape[1]\n",
        "input_img = Input(shape=(image_size, image_size, 3))\n",
        "x = Conv2D(64, (3, 3), padding='same')(input_img)\n",
        "x = Activation('relu')(x)\n",
        "\n",
        "for i in range(15):\n",
        "    x = Conv2D(64, (3, 3), padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "x = Conv2D(3, (3, 3), padding='same')(x)\n",
        "output_img = Activation('tanh')(x)\n",
        "\n",
        "model = Model(input_img, output_img)\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2JfFrwP1UJH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "es_cb = EarlyStopping(monitor='val_accuracy', patience=2, verbose=1, mode='auto')\n",
        "chkpt = \"model.h5\"\n",
        "cp_cb = ModelCheckpoint(filepath = chkpt, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X764MaQy2EHM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32\n",
        "epochs = 100\n",
        "history = model.fit(X_noise_train, X_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(X_noise_val, X_val),\n",
        "                    callbacks=[es_cb, cp_cb],\n",
        "                    shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9a08n8S2jFE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential, load_model\n",
        "\n",
        "model = load_model('model.h5')\n",
        "result = model.predict(X_test_noise) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HD-uyzqK3gpC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Display the 1st 8 corrupted and denoised images\n",
        "rows, cols = 1, 3\n",
        "num = rows * cols\n",
        "imgs = np.concatenate([X_test[:num], X_test_noise[:num], result[:num]])\n",
        "imgs = imgs.reshape((rows * 3, cols, image_size, image_size, image_channels))\n",
        "imgs = np.vstack(np.split(imgs, rows, axis=1))\n",
        "imgs = imgs.reshape((rows * 3, -1, image_size, image_size, image_channels))\n",
        "imgs = np.vstack([np.hstack(i) for i in imgs])\n",
        "imgs = (imgs * 255).astype(np.uint8)\n",
        "plt.figure()\n",
        "plt.axis('off')\n",
        "# plt.title('Original images: top rows, '\n",
        "#           'Corrupted Input: middle rows, '\n",
        "#           'Denoised Input:  third rows')\n",
        "# plt.imshow(imgs, interpolation='none', cmap='gray')\n",
        "# Image.fromarray(imgs).save('corrupted_and_denoised.png')\n",
        "plt.imshow(imgs)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GluoQAH5sQk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cnn_model():\n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1), input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3),\n",
        "                activation='relu'))\n",
        "    model.add(BatchNormalization()) # Normalize the activations of the previous layer at each batch\n",
        "    model.add(Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1), activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "\n",
        "    model.add(Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "\n",
        "    model.add(Conv2D(filters=128, kernel_size=(3, 3), strides=(1, 1), activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=128, kernel_size=(3, 3), strides=(1, 1), activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "\n",
        "    model.add(Flatten()) # Flatten the input\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dense(12, activation='softmax'))\n",
        "    # Configure the learning process\n",
        "    # The loss function is the objective that the model will try to minimize\n",
        "    # For any classification problem, use accuracy metric\n",
        "    optimizer = Adam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=0.1, decay=0.0)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    \n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7ZEMLWmD6pL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result_train = model.predict(X_noise_train)\n",
        "result_val = model.predict(X_noise_val)\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 100\n",
        "\n",
        "# def train():\n",
        "#     CNN_model = cnn_model()\n",
        "#     annealer = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=5, verbose=1, min_lr=1e-5)\n",
        "#     checkpoint = ModelCheckpoint('model_cnn.h5', verbose=1, save_best_only=True)\n",
        "\n",
        "#     # es_cb = EarlyStopping(monitor='val_accuracy', patience=2, verbose=1, mode='auto')\n",
        "#     # cp_cb = ModelCheckpoint(filepath = chkpt, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
        "\n",
        "#     # Generates batches of image data with data augmentation\n",
        "#     datagen = ImageDataGenerator(rotation_range=360, # Degree range for random rotations\n",
        "#                             width_shift_range=0.2, # Range for random horizontal shifts\n",
        "#                             height_shift_range=0.2, # Range for random vertical shifts\n",
        "#                             zoom_range=0.2, # Range for random zoom\n",
        "#                             horizontal_flip=True, # Randomly flip inputs horizontally\n",
        "#                             vertical_flip=True) # Randomly flip inputs vertically\n",
        "    \n",
        "#     datagen.fit(result_train)\n",
        "#     # Fits the model on batches with real-time data augmentation\n",
        "#     hist = CNN_model.fit_generator(datagen.flow(result_train, Y_noise_train, batch_size=BATCH_SIZE),\n",
        "#                    steps_per_epoch=result_train.shape[0] // BATCH_SIZE,\n",
        "#                    epochs=EPOCHS,\n",
        "#                    verbose=2,\n",
        "#                    callbacks=[annealer, checkpoint],\n",
        "#                    validation_data=(result_val, Y_noise_val))\n",
        "    \n",
        "\n",
        "# # For orignal\n",
        "# def train():\n",
        "#     CNN_model = cnn_model()\n",
        "#     annealer = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=5, verbose=1, min_lr=1e-5)\n",
        "#     checkpoint = ModelCheckpoint('model_cnn_orig.h5', verbose=1, save_best_only=True)\n",
        "\n",
        "#     # es_cb = EarlyStopping(monitor='val_accuracy', patience=2, verbose=1, mode='auto')\n",
        "#     # cp_cb = ModelCheckpoint(filepath = chkpt, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
        "\n",
        "#     # Generates batches of image data with data augmentation\n",
        "#     datagen = ImageDataGenerator(rotation_range=360, # Degree range for random rotations\n",
        "#                             width_shift_range=0.2, # Range for random horizontal shifts\n",
        "#                             height_shift_range=0.2, # Range for random vertical shifts\n",
        "#                             zoom_range=0.2, # Range for random zoom\n",
        "#                             horizontal_flip=True, # Randomly flip inputs horizontally\n",
        "#                             vertical_flip=True) # Randomly flip inputs vertically\n",
        "    \n",
        "#     datagen.fit(X_train)\n",
        "#     # Fits the model on batches with real-time data augmentation\n",
        "#     hist = CNN_model.fit_generator(datagen.flow(X_train, Y_train, batch_size=BATCH_SIZE),\n",
        "#                    steps_per_epoch=X_train.shape[0] // BATCH_SIZE,\n",
        "#                    epochs=EPOCHS,\n",
        "#                    verbose=2,\n",
        "#                    callbacks=[annealer, checkpoint],\n",
        "#                    validation_data=(X_val, Y_val))\n",
        "    \n",
        "def train():\n",
        "    CNN_model = cnn_model()\n",
        "    annealer = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=5, verbose=1, min_lr=1e-5)\n",
        "    checkpoint = ModelCheckpoint('model_cnn_noise.h5', verbose=1, save_best_only=True)\n",
        "\n",
        "    # es_cb = EarlyStopping(monitor='val_accuracy', patience=2, verbose=1, mode='auto')\n",
        "    # cp_cb = ModelCheckpoint(filepath = chkpt, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
        "\n",
        "    # Generates batches of image data with data augmentation\n",
        "    datagen = ImageDataGenerator(rotation_range=360, # Degree range for random rotations\n",
        "                            width_shift_range=0.2, # Range for random horizontal shifts\n",
        "                            height_shift_range=0.2, # Range for random vertical shifts\n",
        "                            zoom_range=0.2, # Range for random zoom\n",
        "                            horizontal_flip=True, # Randomly flip inputs horizontally\n",
        "                            vertical_flip=True) # Randomly flip inputs vertically\n",
        "    \n",
        "    datagen.fit(X_noise_train)\n",
        "    # Fits the model on batches with real-time data augmentation\n",
        "    hist = CNN_model.fit_generator(datagen.flow(X_noise_train, Y_noise_train, batch_size=BATCH_SIZE),\n",
        "                   steps_per_epoch=X_noise_train.shape[0] // BATCH_SIZE,\n",
        "                   epochs=EPOCHS,\n",
        "                   verbose=2,\n",
        "                   callbacks=[annealer, checkpoint],\n",
        "                   validation_data=(X_noise_val, Y_noise_val))\n",
        "train()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyB_PZ3SFUtk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result_test = model.predict(X_test_noise)\n",
        "\n",
        "final_model = load_model('model_cnn.h5')\n",
        "final_loss, final_accuracy = final_model.evaluate(result_val, Y_noise_val)\n",
        "print('Final Loss: {}, Final Accuracy: {}'.format(final_loss, final_accuracy))\n",
        "\n",
        "predictions = final_model.predict(result_test)\n",
        "predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "df = pd.DataFrame({'file': [file for file in test['File'].values], 'species': [species[i] for i in predictions]})\n",
        "df.to_csv('submission.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxyYgKGsF-NW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_NWobEjfjPR",
        "colab_type": "text"
      },
      "source": [
        "**Orignal data without noise**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNunmEmkHlHL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_orig_model = load_model('model_cnn_orig.h5')\n",
        "final_orig_loss, final_orig_accuracy = final_orig_model.evaluate(X_val, Y_val)\n",
        "print('Final Loss: {}, Final Accuracy: {}'.format(final_orig_loss, final_orig_accuracy))\n",
        "\n",
        "predictions = final_orig_model.predict(X_test)\n",
        "predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "df = pd.DataFrame({'file': [file for file in test['File'].values], 'species': [species[i] for i in predictions]})\n",
        "df.to_csv('submission.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaIPjx10fvvm",
        "colab_type": "text"
      },
      "source": [
        "**data with noise**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqCVh401fys7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_noise_model = load_model('model_cnn_noise.h5')\n",
        "final_noise_loss, final_noise_accuracy = final_noise_model.evaluate(X_noise_val, Y_noise_val)\n",
        "print('Final Loss: {}, Final Accuracy: {}'.format(final_noise_loss, final_noise_accuracy))\n",
        "\n",
        "predictions = final_noise_model.predict(X_test_noise)\n",
        "predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "df = pd.DataFrame({'file': [file for file in test['File'].values], 'species': [species[i] for i in predictions]})\n",
        "df.to_csv('submission.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}